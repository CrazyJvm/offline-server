app {
  database {
    url = "jdbc:h2:mem:test1"
    driver = org.h2.Driver
    connectionPool = disabled
    keepAliveConnection = true
  }

  kafka {
    bootstrap.servers: "localhost:9092"
    group.id: "jobsubmitter"
    fetch.maxRetries: "1"
    fetch.maxBytes: "10000"
  }

  timeZone: "Asia/Shanghai"

  jobs: [
    {
      name: "test-hourly-jobs1"
      submitInfo {
        interval: hourly
        topicAndGroups: [
          { group: "test-group",              topic: "test-topic" }
        ]
      }
      jobInfo {
        terminalJob: "io.growing.test.TestJob2"
        jobType: "debug", priority: 3, properties { typ: "hour" }
        jobGraph: [
          {
            job: "io.growing.test.TestJob0"
            next: [ "io.growing.test.TestJob1", "io.growing.test.TestJob2" ]
          }, {
            job: "io.growing.test.TestJob1"
            next: [ "io.growing.test.TestJob3" ]
          }, {
            job: "io.growing.test.TestJob2"
            next: [ "io.growing.test.TestJob3", "io.growing.test.TestJob4" ]
          }
        ]
      }
    }
    {
      name: "test-hourly-jobs2"
      submitInfo {
        interval: hourly
        parent: "test-hourly-jobs1"
      }
      jobInfo {
        jobType: "debug", priority: 3, properties { typ: "hour" }
        jobGraph: [
          {
            job: "io.growing.test.TestJob10", properties { ai: "ai1" }
            next: [ "io.growing.test.TestJob11" ]
          }, {
            job: "io.growing.test.TestJob10", properties { ai: "ai2" }
            next: [ "io.growing.test.TestJob11" ]
          }
        ]
      }
    }
  ]

  submitter {
    interval: 120
  }

  scheduler {
    // failed job 重试的次数, debug的时候不重试
    maxRetries: 0

    jobServer {
      debug: true

      sparkConfigs {
        spark.master: "local[2]"
        spark.app.name: "local-JobServer"
        spark.sql.warehouse.dir: "file:///tmp/spark-warehouse"
        spark.ui.enabled: "false"
        spark.sql.test: ""
      }
      jobContext {


      }
    }
  }

}

akka {
  actor {
    deployment {
      "/scheduler/jobServer/jobRouter" {
        dispatcher: "jobDispatcher"
        router: "round-robin-pool"
        nr-of-instances: 3
      }
      "/scheduler/jobServer/jobRouter/*" {
        mailbox: "jobPriorityQueue"
      }
    }
  }
}

jobDispatcher {
  type: "Dispatcher"
  executor: "fork-join-executor"
  fork-join-executor {
    parallelism-min: 2
    parallelism-factor: 2.0
    parallelism-max: 4
    task-peeking-mode: FIFO     // 设置FIFO模式
  }
  throughput: 10
}

jobPriorityQueue {
  mailbox-type: "io.growing.offline.models.JobPriorityQueue"
}